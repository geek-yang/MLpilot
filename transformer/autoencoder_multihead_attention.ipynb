{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder with multi-head attention using pytorch\n",
    "Following the example from: <br>\n",
    "https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding the scaled dot-product attention\n",
    "![attention](../assets/img/multihead_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![equation](../assets/img/attention.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = query.size(-1) ** 0.5\n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding the multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_k)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(\n",
    "    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** (dim / dim_model))\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The encoder-decoder architecture of transformer\n",
    "![transformer](../assets/img/transformer.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b197f2d146ba9dade075c512c2d967b6caa99253af0cd5aa569ca8174c9a813e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
